{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T18:18:19.515170Z",
     "iopub.status.busy": "2025-08-23T18:18:19.514879Z",
     "iopub.status.idle": "2025-08-23T18:27:42.058831Z",
     "shell.execute_reply": "2025-08-23T18:27:42.057981Z",
     "shell.execute_reply.started": "2025-08-23T18:18:19.515143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hTotal: 1875 image-mask pairs\n",
      "Train: 1312, Val: 281, Test: 282\n",
      "\n",
      "ğŸš€ Training with encoder: pvt_v2_b3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e343956df84491b961d1f0f75d934e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/181M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea94d2f2a7b453baff4d0b22cfdc28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16cf77ccb7f41b1bc19bff2e443df97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training PVT_V2_B3 for 20 epochs ===\n",
      "Epoch 01/20 | Train Loss: 1.2519 | Val Loss: 0.8787\n",
      "Epoch 02/20 | Train Loss: 0.6784 | Val Loss: 0.5814\n",
      "Epoch 03/20 | Train Loss: 0.4643 | Val Loss: 0.4864\n",
      "Epoch 04/20 | Train Loss: 0.3590 | Val Loss: 0.3748\n",
      "Epoch 05/20 | Train Loss: 0.3044 | Val Loss: 0.3671\n",
      "Epoch 06/20 | Train Loss: 0.2662 | Val Loss: 0.3654\n",
      "Epoch 07/20 | Train Loss: 0.2235 | Val Loss: 0.3786\n",
      "Epoch 08/20 | Train Loss: 0.1973 | Val Loss: 0.3848\n",
      "Epoch 09/20 | Train Loss: 0.1814 | Val Loss: 0.4635\n",
      "Epoch 10/20 | Train Loss: 0.1828 | Val Loss: 0.4636\n",
      "Epoch 11/20 | Train Loss: 0.1693 | Val Loss: 0.3421\n",
      "Epoch 12/20 | Train Loss: 0.1600 | Val Loss: 0.5839\n",
      "Epoch 13/20 | Train Loss: 0.2709 | Val Loss: 0.2966\n",
      "Epoch 14/20 | Train Loss: 0.1779 | Val Loss: 0.3113\n",
      "Epoch 15/20 | Train Loss: 0.1564 | Val Loss: 0.3238\n",
      "Epoch 16/20 | Train Loss: 0.1364 | Val Loss: 0.3107\n",
      "Epoch 17/20 | Train Loss: 0.1276 | Val Loss: 0.3381\n",
      "Epoch 18/20 | Train Loss: 0.1192 | Val Loss: 0.3273\n",
      "Epoch 19/20 | Train Loss: 0.1190 | Val Loss: 0.3765\n",
      "Epoch 20/20 | Train Loss: 0.1107 | Val Loss: 0.3708\n",
      "\n",
      "ğŸ“Š Final Comparison Results:\n",
      "       Model  precision    recall        f1       iou      dice\n",
      "0  pvt_v2_b3   0.858722  0.933257  0.892836  0.809776  0.892836\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "import timm\n",
    "\n",
    "# =========================\n",
    "# Repro\n",
    "# =========================\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class BUSIDataset(Dataset):\n",
    "    def __init__(self, df, resize=(256, 256)):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image\"]\n",
    "        mask_path = self.df.iloc[idx][\"mask\"]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.resize is not None:\n",
    "            img = img.resize(self.resize)\n",
    "            mask = mask.resize(self.resize)\n",
    "\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        mask = (np.array(mask, dtype=np.float32) / 255.0)\n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        return img, mask\n",
    "\n",
    "# =========================\n",
    "# DATASET PATHS (EDIT THIS)\n",
    "# =========================\n",
    "IMG_DIR  = \"path/to/BUSI/Images\"\n",
    "MASK_DIR = \"path/to/BUSI/Masks\"\n",
    "\n",
    "images = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n",
    "masks  = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "\n",
    "assert len(images) == len(masks) and len(images) > 0, \"Images/Masks count mismatch or empty!\"\n",
    "\n",
    "df = pd.DataFrame({\"image\": images, \"mask\": masks})\n",
    "print(f\"Total: {len(df)} image-mask pairs\")\n",
    "\n",
    "\n",
    "\n",
    "# Train/Val/Test = 70/15/15\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, shuffle=True)\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.50, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BUSIDataset(train_df)\n",
    "val_dataset   = BUSIDataset(val_df)\n",
    "test_dataset  = BUSIDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# =========================\n",
    "# TIMM Encoder Wrapper\n",
    "# =========================\n",
    "class TimmEncoderWrapper(nn.Module):\n",
    "    def __init__(self, model_name, in_channels=3, depth=5, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=in_channels\n",
    "        )\n",
    "        self._out_channels = self.model.feature_info.channels()[:depth]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[:len(self._out_channels)]\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self._out_channels\n",
    "\n",
    "# List of TIMM backbones to test\n",
    "timm_encoders = [\n",
    "    \"swin_tiny_patch4_window7_224\", \"swin_base_patch4_window7_224\",\n",
    "    \"pvt_v2_b0\", \"pvt_v2_b2\",\"pvt_v2_b3\",\n",
    "    \"vit_base_patch16_224\", \"vit_small_patch32_224\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Loss + Metrics\n",
    "# =========================\n",
    "def weighted_bce_dice_loss(pred, target):\n",
    "    t = target.view(-1)\n",
    "    num_pos = torch.clamp(t.sum(), min=0.)\n",
    "    num_tot = t.numel()\n",
    "    num_neg = num_tot - num_pos\n",
    "    pos_weight = (num_neg / (num_pos + 1e-6)).to(target.device) if num_pos > 0 else torch.tensor(1.0, device=target.device)\n",
    "\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, pos_weight=pos_weight)\n",
    "    p = torch.sigmoid(pred)\n",
    "    intersection = (p * target).sum(dim=(1,2,3))\n",
    "    denom = p.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3)) + 1e-6\n",
    "    dice = (2.0 * intersection + 1e-6) / denom\n",
    "    dice_loss = 1.0 - dice.mean()\n",
    "    return bce + dice_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_from_logits(pred_logits, masks):\n",
    "    probs = torch.sigmoid(pred_logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "    TP = (preds * masks).sum().item()\n",
    "    FP = (preds * (1 - masks)).sum().item()\n",
    "    FN = ((1 - preds) * masks).sum().item()\n",
    "    eps = 1e-7\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    recall    = TP / (TP + FN + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    iou       = TP / (TP + FP + FN + eps)\n",
    "    dice      = (2 * TP) / (2 * TP + FP + FN + eps)\n",
    "    return precision, recall, f1, iou, dice\n",
    "\n",
    "# =========================\n",
    "# Train / Eval\n",
    "# =========================\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device).float()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(imgs)\n",
    "        loss = weighted_bce_dice_loss(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss(model, loader, device):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device).float()\n",
    "        logits = model(imgs)\n",
    "        loss = weighted_bce_dice_loss(logits, masks)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    totals = {\"precision\":0.0, \"recall\":0.0, \"f1\":0.0, \"iou\":0.0, \"dice\":0.0}\n",
    "    n = 0\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device).float()\n",
    "        logits = model(imgs)\n",
    "        p, r, f1, iou, d = compute_metrics_from_logits(logits, masks)\n",
    "        totals[\"precision\"] += p\n",
    "        totals[\"recall\"]    += r\n",
    "        totals[\"f1\"]        += f1\n",
    "        totals[\"iou\"]       += iou\n",
    "        totals[\"dice\"]      += d\n",
    "        n += 1\n",
    "    for k in totals:\n",
    "        totals[k] /= max(1, n)\n",
    "    return totals\n",
    "\n",
    "# =========================\n",
    "# Training loop\n",
    "# =========================\n",
    "def run_training(encoder_name, train_loader, val_loader, test_loader, device, epochs=20, lr=1e-4):\n",
    "    encoder = TimmEncoderWrapper(encoder_name)\n",
    "    model = smp.Unet(\n",
    "        encoder=encoder,    # pass encoder object directly\n",
    "        in_channels=3,\n",
    "        classes=1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"\\n=== Training {encoder_name.upper()} for {epochs} epochs ===\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        va_loss = eval_loss(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | Train Loss: {tr_loss:.4f} | Val Loss: {va_loss:.4f}\")\n",
    "\n",
    "    test_metrics = evaluate_metrics(model, test_loader, device)\n",
    "    return model, test_metrics\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run experiments\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "results = []\n",
    "for enc in timm_encoders:\n",
    "    print(f\"\\nğŸš€ Training with encoder: {enc}\")\n",
    "    _, test_metrics = run_training(enc, train_loader, val_loader, test_loader, device, epochs=20)\n",
    "    results.append({\"Model\": enc, **test_metrics})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nğŸ“Š Final Comparison Results:\")\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6972772,
     "sourceId": 12403154,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
